NO XAI + NO HYPERPARAMETER TUNING
APPROACH 1: Splitting randomly (70-30)

Accuracy for XGBoost: 74.80%
              precision    recall  f1-score   support

           0       0.71      0.77      0.74      1134
           1       0.74      0.64      0.69       437
           2       0.81      0.46      0.58       230
           3       0.79      0.84      0.81      1028

    accuracy                           0.75      2829
   macro avg       0.76      0.67      0.70      2829
weighted avg       0.75      0.75      0.74      2829
Accuracy for Random Forest: 72.50%
              precision    recall  f1-score   support

           0       0.67      0.77      0.72      1134
           1       0.79      0.59      0.67       437
           2       0.78      0.32      0.46       230
           3       0.76      0.82      0.79      1028

    accuracy                           0.72      2829
   macro avg       0.75      0.63      0.66      2829
weighted avg       0.73      0.72      0.72      2829
Accuracy for CatBoost: 74.34%
              precision    recall  f1-score   support

           0       0.70      0.77      0.73      1134
           1       0.78      0.63      0.70       437
           2       0.73      0.45      0.56       230
           3       0.78      0.83      0.80      1028

    accuracy                           0.74      2829
   macro avg       0.75      0.67      0.70      2829
weighted avg       0.74      0.74      0.74      2829

APPROACH 2 Splitting subject wise (70-30)

Accuracy for XGBoost: 73.17%
              precision    recall  f1-score   support

           0       0.69      0.78      0.73      1131
           1       0.73      0.56      0.64       434
           2       0.75      0.48      0.59       242
           3       0.78      0.81      0.80      1026

    accuracy                           0.73      2833
   macro avg       0.74      0.66      0.69      2833
weighted avg       0.73      0.73      0.73      2833
Accuracy for Random Forest: 73.42%
              precision    recall  f1-score   support

           0       0.68      0.81      0.74      1131
           1       0.79      0.54      0.64       434
           2       0.78      0.36      0.50       242
           3       0.78      0.82      0.80      1026

    accuracy                           0.73      2833
   macro avg       0.76      0.63      0.67      2833
weighted avg       0.74      0.73      0.73      2833
Accuracy for CatBoost: 73.31%
              precision    recall  f1-score   support

           0       0.69      0.77      0.73      1131
           1       0.72      0.60      0.65       434
           2       0.74      0.43      0.55       242
           3       0.79      0.82      0.80      1026

    accuracy                           0.73      2833
   macro avg       0.73      0.66      0.68      2833
weighted avg       0.73      0.73      0.73      2833

ONLY HYPERPARAMETER TUNING

XGBOOST PARAMS himika.tasnim@g.bracu.ac.bd
objective='multi:softmax',
eval_metric='mlogloss',
random_state=150,
subsample=0.9,
reg_lambda=0.04,
reg_alpha=0.7,
n_estimators=600,
max_depth=9,
learning_rate=0.05,
gamma=0,
colsample_bytree=0.9

Accuracy for best XGBoost: 75.15%
              precision    recall  f1-score   support

           0       0.71      0.77      0.74      1134
           1       0.78      0.62      0.69       437
           2       0.77      0.45      0.57       230
           3       0.79      0.85      0.82      1028

    accuracy                           0.75      2829
   macro avg       0.76      0.67      0.70      2829
weighted avg       0.75      0.75      0.75      2829

CATBOOST PARAMS reshad.ul.karim@g.bracu.ac.bd
best_params = {
    'bagging_temperature': 0.41750608402789213,
    'boosting_type': 'Ordered',
    'bootstrap_type': 'MVS',
    'border_count': 65,
    'class_weights': {0: 0.15504341779626502, 1: 0.31884044106753684, 2: 0.1498282622340735, 3: 0.3762878789021247},
    'depth': 9,
    'grow_policy': 'SymmetricTree',
    'iterations': 858,
    'l2_leaf_reg': 2.8547290957672247,
    'leaf_estimation_iterations': 2,
    'leaf_estimation_method': 'Gradient',
    'learning_rate': 0.28977697282757586,
    'min_data_in_leaf': 7,
    'od_type': 'Iter',
    'od_wait': 38,
    'random_strength': 0.7671882889311269,
    'rsm': 0.46173782224831794,
    'score_function': 'Cosine'
}

Test set accuracy: 75.29%
              precision    recall  f1-score   support

           0       0.73      0.75      0.74       762
           1       0.76      0.66      0.71       300
           2       0.83      0.50      0.62       163
           3       0.76      0.86      0.81       661

    accuracy                           0.75      1886
   macro avg       0.77      0.69      0.72      1886
weighted avg       0.76      0.75      0.75      1886
RFC PARAMS abrar.samin@g.bracu.ac.bd
Accuracy for best Random Forest: 74.41%
              precision    recall  f1-score   support

           0       0.70      0.77      0.73      1134
           1       0.80      0.63      0.71       437
           2       0.79      0.40      0.53       230
           3       0.77      0.84      0.81      1028

    accuracy                           0.74      2829
   macro avg       0.77      0.66      0.69      2829
weighted avg       0.75      0.74      0.74      2829
RandomForestClassifier(
    n_estimators=600,
    min_samples_split=5,
    min_samples_leaf=1,
    max_features='sqrt',
    max_depth=20,
    criterion='entropy',
    bootstrap=False,
    random_state=90
)

SVM PARAMS reshad.ul.karim@g.bracu.ac.bd
best_svm_model = Pipeline([
    ('scaler', StandardScaler()),
    ('svm', SVC(
        tol=0.12, #0.12
        shrinking=False,
        kernel='rbf',
        gamma='scale',
        degree=3,
        coef0=0.5,
        class_weight='balanced',
        C=100.0,
        random_state=rnd

Accuracy for best SVM: 70.20%
              precision    recall  f1-score   support

           0       0.72      0.66      0.69      1131
           1       0.58      0.71      0.64       433
           2       0.59      0.66      0.62       242
           3       0.78      0.76      0.77      1023

    accuracy                           0.70      2829
   macro avg       0.67      0.70      0.68      2829
weighted avg       0.71      0.70      0.70      2829